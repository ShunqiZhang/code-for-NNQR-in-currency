{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52d32a49-685c-4e6b-9908-943ca6c5d7cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt \n",
    "from arch import arch_model\n",
    "from scipy.stats import norm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "import logging\n",
    "import warnings\n",
    "import random\n",
    "import networkx as nx\n",
    "from caviar import *\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Setting random seeds\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Setting the CuDNN in deterministic mode\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# If multithreading or multiprocessing is used, make sure that these also use the same random seeds\n",
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01d1086f-7fb9-4a12-9304-71b5efc1a817",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date       AUD       EUR       GBP       BRL       CAD       CNY  \\\n",
      "0 2018-01-03  0.334863 -0.362698 -0.962935  0.667124 -0.495991 -0.530372   \n",
      "1 2018-01-04  0.334211  0.656456  0.227988  0.509819  0.240629  0.542035   \n",
      "2 2018-01-05  0.074953 -0.456648  0.306200 -0.001248  2.406126  0.239796   \n",
      "3 2018-01-08 -0.403445 -1.234715  0.057436 -0.039856 -0.519592 -0.503176   \n",
      "4 2018-01-09 -0.304518 -0.955438 -0.623592 -0.553009 -0.613347 -1.587930   \n",
      "\n",
      "        INR       JPY       KRW  ...       ZAR       BTC       ETH       XRP  \\\n",
      "0 -0.145050 -0.129262 -0.635381  ...  0.888510  0.195459  1.586825  3.628685   \n",
      "1  0.543494 -0.776700  0.317166  ...  0.283870  0.108466 -0.228345  0.292872   \n",
      "2  0.150080 -0.612152  0.285386  ... -0.238303  2.589548  0.280511 -0.718810   \n",
      "3 -0.440428  0.194247 -1.074947  ... -0.367659 -2.547256  3.853138 -0.502358   \n",
      "4 -0.832109  0.857014 -0.421764  ...  0.066096 -0.799821  0.295780 -1.401472   \n",
      "\n",
      "        BNB      DOGE       ADA        TRX       XLM      LINK  \n",
      "0  1.088909 -0.010672  4.879945   2.686583  7.567781  0.077053  \n",
      "1 -0.636529  0.475629  0.581657  12.095426 -3.355267  5.054912  \n",
      "2  7.904958  3.150200 -1.699495   0.508360 -1.327039 -0.815836  \n",
      "3  3.670200  2.285333 -1.616259  -5.304691 -0.781060  5.360687  \n",
      "4 -0.728853 -1.416579 -1.569825  -3.476186 -1.093522 -1.468596  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "Index(['AUD', 'EUR', 'GBP', 'BRL', 'CAD', 'CNY', 'INR', 'JPY', 'KRW', 'MXN',\n",
      "       'ZAR', 'BTC', 'ETH', 'XRP', 'BNB', 'DOGE', 'ADA', 'TRX', 'XLM', 'LINK'],\n",
      "      dtype='object')\n",
      "           date       AUD       EUR       GBP       BRL       CAD       CNY  \\\n",
      "0    2018-01-03  0.334863 -0.362698 -0.962935  0.667124 -0.495991 -0.530372   \n",
      "1    2018-01-04  0.334211  0.656456  0.227988  0.509819  0.240629  0.542035   \n",
      "2    2018-01-05  0.074953 -0.456648  0.306200 -0.001248  2.406126  0.239796   \n",
      "3    2018-01-08 -0.403445 -1.234715  0.057436 -0.039856 -0.519592 -0.503176   \n",
      "4    2018-01-09 -0.304518 -0.955438 -0.623592 -0.553009 -0.613347 -1.587930   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "1695 2024-10-17  0.951873 -0.738979  0.114249 -0.262937 -0.312019 -0.100826   \n",
      "1696 2024-10-18 -0.031487  0.371217  0.195929 -0.007935 -0.208785  1.029334   \n",
      "1697 2024-10-21 -0.898234 -0.698279 -0.567894 -0.402437 -0.721574 -0.854682   \n",
      "1698 2024-10-22  0.437467 -0.300448 -0.145256  0.238558  0.304633 -0.186105   \n",
      "1699 2024-10-23 -1.089745 -0.532745 -0.337128 -0.204831 -0.122709 -0.090734   \n",
      "\n",
      "           INR       JPY       KRW  ...       ZAR       BTC       ETH  \\\n",
      "0    -0.145050 -0.129262 -0.635381  ...  0.888510  0.195459  1.586825   \n",
      "1     0.543494 -0.776700  0.317166  ...  0.283870  0.108466 -0.228345   \n",
      "2     0.150080 -0.612152  0.285386  ... -0.238303  2.589548  0.280511   \n",
      "3    -0.440428  0.194247 -1.074947  ... -0.367659 -2.547256  3.853138   \n",
      "4    -0.832109  0.857014 -0.421764  ...  0.066096 -0.799821  0.295780   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "1695 -0.059821 -0.465327 -0.985952  ... -0.214010 -0.120133 -0.068367   \n",
      "1696  0.088750  0.689939  0.227609  ...  0.593797  0.352242  0.251560   \n",
      "1697 -0.096920 -1.048106 -1.280341  ... -0.338786 -0.366663  0.162530   \n",
      "1698  0.088737 -0.740440  0.018631  ...  0.782635 -0.035903 -0.329716   \n",
      "1699  0.051611 -2.005876 -0.298834  ... -1.938378 -0.242220 -0.660022   \n",
      "\n",
      "           XRP       BNB      DOGE       ADA        TRX       XLM      LINK  \n",
      "0     3.628685  1.088909 -0.010672  4.879945   2.686583  7.567781  0.077053  \n",
      "1     0.292872 -0.636529  0.475629  0.581657  12.095426 -3.355267  5.054912  \n",
      "2    -0.718810  7.904958  3.150200 -1.699495   0.508360 -1.327039 -0.815836  \n",
      "3    -0.502358  3.670200  2.285333 -1.616259  -5.304691 -0.781060  5.360687  \n",
      "4    -1.401472 -0.728853 -1.416579 -1.569825  -3.476186 -1.093522 -1.468596  \n",
      "...        ...       ...       ...       ...        ...       ...       ...  \n",
      "1695 -0.104150 -0.282476  0.297121 -0.418878  -0.033564 -0.045404 -0.278932  \n",
      "1696  0.084429  0.153969  0.720943  0.341520  -0.082500  0.236993  0.502859  \n",
      "1697 -0.013283 -0.029826  0.558317  0.478234  -0.050218 -0.123733  0.252891  \n",
      "1698 -0.312409 -0.206029 -0.379679  0.123325   0.152796  0.006306  0.295246  \n",
      "1699 -0.208293 -0.289041  0.008539 -0.584815   0.010524 -0.084079 -0.690248  \n",
      "\n",
      "[1700 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "data = pd.read_excel('estimate_data.xlsx')\n",
    "\n",
    "# Exclude date columns\n",
    "numeric_cols = data.columns.difference(['date'])\n",
    "\n",
    "des_data=data.iloc[:, 1:21]\n",
    "# Calculate descriptive statistics\n",
    "desc_stats = des_data.describe()\n",
    "# Calculate skewness and kurtosis\n",
    "skewness = des_data.apply(skew)\n",
    "kurtosis_values = des_data.apply(kurtosis)\n",
    "# Calculate the Jarque-Bera statistic\n",
    "jb_stats = des_data.apply(lambda x: jarque_bera(x)[0])\n",
    "# Combine all statistics into a new DataFrame\n",
    "all_stats = pd.concat([desc_stats, skewness, kurtosis_values, jb_stats], axis=1)\n",
    "# Create a new DataFrame to store all the stats\n",
    "stats_df = pd.DataFrame()\n",
    "# Add columns for each statistic\n",
    "stats_df = pd.DataFrame({\n",
    "    'mean': desc_stats.loc['mean'],\n",
    "    'std': desc_stats.loc['std'],\n",
    "    'min': desc_stats.loc['min'],\n",
    "    '25%': desc_stats.loc['25%'],\n",
    "    '50%': desc_stats.loc['50%'],\n",
    "    '75%': desc_stats.loc['75%'],\n",
    "    'max': desc_stats.loc['max'],\n",
    "    'skewness': skewness,\n",
    "    'kurtosis': kurtosis_values,\n",
    "    'Jarque-Bera': jb_stats\n",
    "})\n",
    "# Reset the index so that the variable name is a column\n",
    "stats_df.reset_index(inplace=True)\n",
    "stats_df.rename(columns={'index': 'Variable'}, inplace=True)\n",
    "# Export results to excel\n",
    "stats_df.to_excel('summary_sta.xlsx', index=False)\n",
    "\n",
    "\n",
    "# Apply standardisation\n",
    "scaler = StandardScaler()\n",
    "data[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n",
    "\n",
    "# View Results\n",
    "print(data.head())\n",
    "# Convert ‘date_x’ to date format and rename to ‘date’\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "vari_list = data.columns[1:]\n",
    "print(vari_list)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d527c4c-67d0-4d73-8328-492962529e8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 1 best initial betas out of 1...\n",
      "Optimizing...\n",
      "when m = 1\n",
      "Update 0: 25.1099239498436\n",
      "Update 1: 11.062978099067212\n",
      "Update 2: 11.062963595016875\n",
      "Update 3: 11.062963595016875\n",
      "Final loss: 11.062963595016875\n",
      "Time taken(s): 1.37\n",
      "Generating 1 best initial betas out of 1...\n",
      "Optimizing...\n",
      "when m = 1\n",
      "Update 0: 35.61201491184071\n",
      "Update 1: 10.78063549070473\n",
      "Update 2: 10.78062869004228\n",
      "Update 3: 10.780628686913094\n",
      "Update 4: 10.780628686913094\n",
      "Final loss: 10.780628686913094\n",
      "Time taken(s): 1.34\n",
      "Generating 1 best initial betas out of 1...\n",
      "Optimizing...\n",
      "when m = 1\n",
      "Update 0: 35.65654983410919\n",
      "Update 1: 10.686460813602059\n",
      "Update 2: 10.686416470637313\n",
      "Update 3: 10.686416470637313\n",
      "Final loss: 10.686416470637313\n",
      "Time taken(s): 1.90\n",
      "Generating 1 best initial betas out of 1...\n",
      "Optimizing...\n",
      "when m = 1\n",
      "Update 0: 12.345178653170079\n",
      "Update 1: 10.69050069927374\n",
      "Update 2: 10.504257345694162\n",
      "Update 3: 10.50420327743734\n",
      "Update 4: 10.504203277437341\n",
      "Final loss: 10.50420327743734\n",
      "Time taken(s): 2.26\n",
      "Generating 1 best initial betas out of 1...\n",
      "Optimizing...\n",
      "when m = 1\n",
      "Update 0: 36.836522068969735\n",
      "Update 1: 11.153906323468515\n",
      "Update 2: 11.153906323459596\n",
      "Final loss: 11.153906323459596\n",
      "Time taken(s): 1.14\n",
      "Generating 1 best initial betas out of 1...\n",
      "Optimizing...\n",
      "when m = 1\n",
      "Update 0: 24.63990423244346\n",
      "Update 1: 11.29899905807809\n",
      "Update 2: 11.298751248993256\n",
      "Update 3: 11.298751248993257\n",
      "Final loss: 11.298751248993256\n",
      "Time taken(s): 1.22\n",
      "Generating 1 best initial betas out of 1...\n",
      "Optimizing...\n",
      "when m = 1\n",
      "Update 0: 46.485545218119825\n",
      "Update 1: 11.46886527139261\n",
      "Update 2: 11.46886527139261\n",
      "Final loss: 11.46886527139261\n",
      "Time taken(s): 0.89\n",
      "Generating 1 best initial betas out of 1...\n",
      "Optimizing...\n",
      "when m = 1\n",
      "Update 0: 15.254229705054271\n",
      "Update 1: 9.669595497636683\n",
      "Update 2: 9.669595142336618\n",
      "Update 3: 9.669595045421804\n",
      "Update 4: 9.669595045421806\n",
      "Final loss: 9.669595045421804\n",
      "Time taken(s): 1.58\n",
      "Generating 1 best initial betas out of 1...\n",
      "Optimizing...\n",
      "when m = 1\n",
      "Update 0: 61.42164319271185\n",
      "Update 1: 9.868705134080917\n",
      "Update 2: 9.868705122774792\n",
      "Update 3: 9.86870511709258\n",
      "Update 4: 9.86870511709258\n",
      "Final loss: 9.86870511709258\n",
      "Time taken(s): 1.76\n",
      "Generating 1 best initial betas out of 1...\n",
      "Optimizing...\n",
      "when m = 1\n",
      "Update 0: 13.13044747496541\n",
      "Update 1: 11.870726925140898\n",
      "Update 2: 11.870726925140904\n",
      "Final loss: 11.870726925140898\n",
      "Time taken(s): 0.74\n",
      "Generating 1 best initial betas out of 1...\n",
      "Optimizing...\n",
      "when m = 1\n",
      "Update 0: 25.516314705749416\n",
      "Update 1: 10.913356777751313\n",
      "Update 2: 10.913356777751314\n",
      "Final loss: 10.913356777751313\n",
      "Time taken(s): 1.52\n",
      "Generating 1 best initial betas out of 1...\n",
      "Optimizing...\n",
      "when m = 1\n",
      "Update 0: 61.167352733893736\n",
      "Update 1: 11.822464758047655\n",
      "Update 2: 11.822464750383908\n",
      "Update 3: 11.822464743617557\n",
      "Update 4: 11.822464743617559\n",
      "Final loss: 11.822464743617557\n",
      "Time taken(s): 1.40\n",
      "Generating 1 best initial betas out of 1...\n",
      "Optimizing...\n",
      "when m = 1\n",
      "Update 0: 27.1661390147892\n",
      "Update 1: 11.868136453384524\n",
      "Update 2: 11.868135801939548\n",
      "Update 3: 11.868135793810724\n",
      "Update 4: 11.868135793810723\n",
      "Final loss: 11.868135793810724\n",
      "Time taken(s): 0.92\n",
      "Generating 1 best initial betas out of 1...\n",
      "Optimizing...\n",
      "when m = 1\n",
      "Update 0: 41.17683960045605\n",
      "Update 1: 10.495910531600527\n",
      "Update 2: 10.49591053160053\n",
      "Final loss: 10.495910531600527\n",
      "Time taken(s): 1.06\n",
      "Generating 1 best initial betas out of 1...\n",
      "Optimizing...\n",
      "when m = 1\n",
      "Update 0: 37.653069235088466\n",
      "Update 1: 10.968398449283558\n",
      "Update 2: 10.968398166766422\n",
      "Update 3: 10.968398013225634\n",
      "Update 4: 10.96839801322564\n",
      "Final loss: 10.968398013225634\n",
      "Time taken(s): 1.24\n",
      "Generating 1 best initial betas out of 1...\n",
      "Optimizing...\n",
      "when m = 1\n",
      "Update 0: 38.46285196854146\n",
      "Update 1: 8.348814482707134\n",
      "Update 2: 8.34881336551651\n",
      "Update 3: 8.348813365516511\n",
      "Final loss: 8.34881336551651\n",
      "Time taken(s): 1.63\n",
      "Generating 1 best initial betas out of 1...\n",
      "Optimizing...\n",
      "when m = 1\n",
      "Update 0: 48.321482413096\n",
      "Update 1: 10.951885897504539\n",
      "Update 2: 10.951885884682989\n",
      "Update 3: 10.951885870632836\n",
      "Update 4: 10.951885870632841\n",
      "Final loss: 10.951885870632836\n",
      "Time taken(s): 1.24\n",
      "Generating 1 best initial betas out of 1...\n",
      "Optimizing...\n",
      "when m = 1\n",
      "Update 0: 68.32677763296547\n",
      "Update 1: 10.191402468774056\n",
      "Update 2: 10.191402449442613\n",
      "Update 3: 10.191402449130166\n",
      "Update 4: 10.191402449130166\n",
      "Final loss: 10.191402449130166\n",
      "Time taken(s): 1.09\n",
      "Generating 1 best initial betas out of 1...\n",
      "Optimizing...\n",
      "when m = 1\n",
      "Update 0: 57.97989205856666\n",
      "Update 1: 10.467241165948508\n",
      "Update 2: 10.46724116594851\n",
      "Final loss: 10.467241165948508\n",
      "Time taken(s): 0.76\n",
      "Generating 1 best initial betas out of 1...\n",
      "Optimizing...\n",
      "when m = 1\n",
      "Update 0: 46.75388972200246\n",
      "Update 1: 11.021039616373866\n",
      "Update 2: 11.021039615623033\n",
      "Update 3: 11.021039615623033\n",
      "Final loss: 11.021039615623033\n",
      "Time taken(s): 1.14\n",
      "       AUD_VaR   EUR_VaR   GBP_VaR   BRL_VaR   CAD_VaR   CNY_VaR   INR_VaR  \\\n",
      "0    -1.572902 -1.229176 -1.280577 -1.380901 -1.038244 -1.382147 -0.320362   \n",
      "1    -1.511607 -1.226847 -1.460787 -1.370054 -1.052523 -1.393658 -0.318621   \n",
      "2    -1.453835 -1.219987 -1.357431 -1.353418 -1.024256 -1.348173 -0.361318   \n",
      "3    -1.372770 -1.222267 -1.282407 -1.316566 -1.138534 -1.260325 -0.354324   \n",
      "4    -1.357222 -1.261439 -1.196094 -1.287200 -1.151558 -1.275346 -0.393323   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1695 -1.548538 -1.211317 -1.470342 -1.498466 -1.084020 -1.607105 -0.396462   \n",
      "1696 -1.552158 -1.227139 -1.353263 -1.478297 -1.075075 -1.486318 -0.378083   \n",
      "1697 -1.462966 -1.214779 -1.267556 -1.431417 -1.054479 -1.507444 -0.362768   \n",
      "1698 -1.526123 -1.228613 -1.338658 -1.432596 -1.094348 -1.593059 -0.351664   \n",
      "1699 -1.478125 -1.223339 -1.276429 -1.398970 -1.068593 -1.495863 -0.337850   \n",
      "\n",
      "       JPY_VaR   KRW_VaR   MXN_VaR   ZAR_VaR   BTC_VaR   ETH_VaR   XRP_VaR  \\\n",
      "0    -1.967672 -1.711896 -1.757329 -1.496603 -0.965267 -0.921753 -0.734290   \n",
      "1    -1.865477 -1.722734 -1.663501 -1.483572 -0.935005 -1.066232 -1.079114   \n",
      "2    -1.843378 -1.644512 -1.587048 -1.452218 -0.897536 -1.031376 -1.074541   \n",
      "3    -1.803756 -1.571572 -1.269031 -1.435233 -1.117968 -0.984176 -1.070366   \n",
      "4    -1.722727 -1.664918 -1.033065 -1.430869 -1.393252 -1.440291 -1.057428   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1695 -1.947273 -1.782596 -2.520957 -1.401500 -1.103762 -0.984157 -0.807831   \n",
      "1696 -1.884728 -1.843671 -1.965399 -1.384633 -1.061084 -0.917953 -0.789106   \n",
      "1697 -1.866011 -1.751234 -1.702085 -1.367547 -1.041412 -0.876093 -0.775332   \n",
      "1698 -1.874930 -1.862230 -2.240903 -1.363666 -1.034738 -0.825253 -0.754175   \n",
      "1699 -1.848125 -1.759093 -2.117834 -1.353458 -0.985008 -0.834511 -0.746219   \n",
      "\n",
      "       BNB_VaR  DOGE_VaR   ADA_VaR   TRX_VaR   XLM_VaR  LINK_VaR  \n",
      "0    -0.911929 -0.922160 -0.891838 -0.213160 -0.630663 -0.841068  \n",
      "1    -1.024626 -0.850855 -1.215702 -0.847719 -1.228322 -0.786722  \n",
      "2    -1.101181 -0.876938 -1.201077 -3.730683 -1.288684 -1.338418  \n",
      "3    -2.199838 -1.409290 -1.361778 -3.403580 -1.292055 -1.395540  \n",
      "4    -2.494720 -1.713391 -1.503924 -4.612560 -1.280444 -1.943484  \n",
      "...        ...       ...       ...       ...       ...       ...  \n",
      "1695 -0.972969 -0.964342 -0.904629 -0.296820 -0.680892 -0.839340  \n",
      "1696 -0.977637 -0.942883 -0.915365 -0.267755 -0.666286 -0.828467  \n",
      "1697 -0.940681 -1.004704 -0.897413 -0.257154 -0.670018 -0.826495  \n",
      "1698 -0.893332 -1.028174 -0.890601 -0.237957 -0.657851 -0.794404  \n",
      "1699 -0.893244 -1.003221 -0.857483 -0.243467 -0.643161 -0.769587  \n",
      "\n",
      "[1700 rows x 20 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create dataframe to store results\n",
    "var_df = pd.DataFrame()\n",
    "\n",
    "# Cycle through the VaR of individual currency exchange rates\n",
    "for col in vari_list:\n",
    "    returns = data[col]*100\n",
    "    caviar_model = CaviarModel(0.05,'asymmetric', 'RQ')\n",
    "    caviar_model.fit(returns)\n",
    "    returns_VaR_predicted = caviar_model.predict(returns)/100\n",
    "    returns_VaR_fitted, returns_VaR_forecast = returns_VaR_predicted[:-1], returns_VaR_predicted[-1]\n",
    "    var_df[col + '_VaR'] = returns_VaR_fitted\n",
    "print(var_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3776f8e9-1210-49c9-84b0-6360118348ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>AUD</th>\n",
       "      <th>EUR</th>\n",
       "      <th>GBP</th>\n",
       "      <th>BRL</th>\n",
       "      <th>CAD</th>\n",
       "      <th>CNY</th>\n",
       "      <th>INR</th>\n",
       "      <th>JPY</th>\n",
       "      <th>KRW</th>\n",
       "      <th>...</th>\n",
       "      <th>ZAR_VaR</th>\n",
       "      <th>BTC_VaR</th>\n",
       "      <th>ETH_VaR</th>\n",
       "      <th>XRP_VaR</th>\n",
       "      <th>BNB_VaR</th>\n",
       "      <th>DOGE_VaR</th>\n",
       "      <th>ADA_VaR</th>\n",
       "      <th>TRX_VaR</th>\n",
       "      <th>XLM_VaR</th>\n",
       "      <th>LINK_VaR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>0.334863</td>\n",
       "      <td>-0.362698</td>\n",
       "      <td>-0.962935</td>\n",
       "      <td>0.667124</td>\n",
       "      <td>-0.495991</td>\n",
       "      <td>-0.530372</td>\n",
       "      <td>-0.145050</td>\n",
       "      <td>-0.129262</td>\n",
       "      <td>-0.635381</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.496603</td>\n",
       "      <td>-0.965267</td>\n",
       "      <td>-0.921753</td>\n",
       "      <td>-0.734290</td>\n",
       "      <td>-0.911929</td>\n",
       "      <td>-0.922160</td>\n",
       "      <td>-0.891838</td>\n",
       "      <td>-0.213160</td>\n",
       "      <td>-0.630663</td>\n",
       "      <td>-0.841068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>0.334211</td>\n",
       "      <td>0.656456</td>\n",
       "      <td>0.227988</td>\n",
       "      <td>0.509819</td>\n",
       "      <td>0.240629</td>\n",
       "      <td>0.542035</td>\n",
       "      <td>0.543494</td>\n",
       "      <td>-0.776700</td>\n",
       "      <td>0.317166</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.483572</td>\n",
       "      <td>-0.935005</td>\n",
       "      <td>-1.066232</td>\n",
       "      <td>-1.079114</td>\n",
       "      <td>-1.024626</td>\n",
       "      <td>-0.850855</td>\n",
       "      <td>-1.215702</td>\n",
       "      <td>-0.847719</td>\n",
       "      <td>-1.228322</td>\n",
       "      <td>-0.786722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>0.074953</td>\n",
       "      <td>-0.456648</td>\n",
       "      <td>0.306200</td>\n",
       "      <td>-0.001248</td>\n",
       "      <td>2.406126</td>\n",
       "      <td>0.239796</td>\n",
       "      <td>0.150080</td>\n",
       "      <td>-0.612152</td>\n",
       "      <td>0.285386</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.452218</td>\n",
       "      <td>-0.897536</td>\n",
       "      <td>-1.031376</td>\n",
       "      <td>-1.074541</td>\n",
       "      <td>-1.101181</td>\n",
       "      <td>-0.876938</td>\n",
       "      <td>-1.201077</td>\n",
       "      <td>-3.730683</td>\n",
       "      <td>-1.288684</td>\n",
       "      <td>-1.338418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>-0.403445</td>\n",
       "      <td>-1.234715</td>\n",
       "      <td>0.057436</td>\n",
       "      <td>-0.039856</td>\n",
       "      <td>-0.519592</td>\n",
       "      <td>-0.503176</td>\n",
       "      <td>-0.440428</td>\n",
       "      <td>0.194247</td>\n",
       "      <td>-1.074947</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.435233</td>\n",
       "      <td>-1.117968</td>\n",
       "      <td>-0.984176</td>\n",
       "      <td>-1.070366</td>\n",
       "      <td>-2.199838</td>\n",
       "      <td>-1.409290</td>\n",
       "      <td>-1.361778</td>\n",
       "      <td>-3.403580</td>\n",
       "      <td>-1.292055</td>\n",
       "      <td>-1.395540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>-0.304518</td>\n",
       "      <td>-0.955438</td>\n",
       "      <td>-0.623592</td>\n",
       "      <td>-0.553009</td>\n",
       "      <td>-0.613347</td>\n",
       "      <td>-1.587930</td>\n",
       "      <td>-0.832109</td>\n",
       "      <td>0.857014</td>\n",
       "      <td>-0.421764</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.430869</td>\n",
       "      <td>-1.393252</td>\n",
       "      <td>-1.440291</td>\n",
       "      <td>-1.057428</td>\n",
       "      <td>-2.494720</td>\n",
       "      <td>-1.713391</td>\n",
       "      <td>-1.503924</td>\n",
       "      <td>-4.612560</td>\n",
       "      <td>-1.280444</td>\n",
       "      <td>-1.943484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1695</th>\n",
       "      <td>2024-10-17</td>\n",
       "      <td>0.951873</td>\n",
       "      <td>-0.738979</td>\n",
       "      <td>0.114249</td>\n",
       "      <td>-0.262937</td>\n",
       "      <td>-0.312019</td>\n",
       "      <td>-0.100826</td>\n",
       "      <td>-0.059821</td>\n",
       "      <td>-0.465327</td>\n",
       "      <td>-0.985952</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.401500</td>\n",
       "      <td>-1.103762</td>\n",
       "      <td>-0.984157</td>\n",
       "      <td>-0.807831</td>\n",
       "      <td>-0.972969</td>\n",
       "      <td>-0.964342</td>\n",
       "      <td>-0.904629</td>\n",
       "      <td>-0.296820</td>\n",
       "      <td>-0.680892</td>\n",
       "      <td>-0.839340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1696</th>\n",
       "      <td>2024-10-18</td>\n",
       "      <td>-0.031487</td>\n",
       "      <td>0.371217</td>\n",
       "      <td>0.195929</td>\n",
       "      <td>-0.007935</td>\n",
       "      <td>-0.208785</td>\n",
       "      <td>1.029334</td>\n",
       "      <td>0.088750</td>\n",
       "      <td>0.689939</td>\n",
       "      <td>0.227609</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.384633</td>\n",
       "      <td>-1.061084</td>\n",
       "      <td>-0.917953</td>\n",
       "      <td>-0.789106</td>\n",
       "      <td>-0.977637</td>\n",
       "      <td>-0.942883</td>\n",
       "      <td>-0.915365</td>\n",
       "      <td>-0.267755</td>\n",
       "      <td>-0.666286</td>\n",
       "      <td>-0.828467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1697</th>\n",
       "      <td>2024-10-21</td>\n",
       "      <td>-0.898234</td>\n",
       "      <td>-0.698279</td>\n",
       "      <td>-0.567894</td>\n",
       "      <td>-0.402437</td>\n",
       "      <td>-0.721574</td>\n",
       "      <td>-0.854682</td>\n",
       "      <td>-0.096920</td>\n",
       "      <td>-1.048106</td>\n",
       "      <td>-1.280341</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.367547</td>\n",
       "      <td>-1.041412</td>\n",
       "      <td>-0.876093</td>\n",
       "      <td>-0.775332</td>\n",
       "      <td>-0.940681</td>\n",
       "      <td>-1.004704</td>\n",
       "      <td>-0.897413</td>\n",
       "      <td>-0.257154</td>\n",
       "      <td>-0.670018</td>\n",
       "      <td>-0.826495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1698</th>\n",
       "      <td>2024-10-22</td>\n",
       "      <td>0.437467</td>\n",
       "      <td>-0.300448</td>\n",
       "      <td>-0.145256</td>\n",
       "      <td>0.238558</td>\n",
       "      <td>0.304633</td>\n",
       "      <td>-0.186105</td>\n",
       "      <td>0.088737</td>\n",
       "      <td>-0.740440</td>\n",
       "      <td>0.018631</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.363666</td>\n",
       "      <td>-1.034738</td>\n",
       "      <td>-0.825253</td>\n",
       "      <td>-0.754175</td>\n",
       "      <td>-0.893332</td>\n",
       "      <td>-1.028174</td>\n",
       "      <td>-0.890601</td>\n",
       "      <td>-0.237957</td>\n",
       "      <td>-0.657851</td>\n",
       "      <td>-0.794404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>2024-10-23</td>\n",
       "      <td>-1.089745</td>\n",
       "      <td>-0.532745</td>\n",
       "      <td>-0.337128</td>\n",
       "      <td>-0.204831</td>\n",
       "      <td>-0.122709</td>\n",
       "      <td>-0.090734</td>\n",
       "      <td>0.051611</td>\n",
       "      <td>-2.005876</td>\n",
       "      <td>-0.298834</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.353458</td>\n",
       "      <td>-0.985008</td>\n",
       "      <td>-0.834511</td>\n",
       "      <td>-0.746219</td>\n",
       "      <td>-0.893244</td>\n",
       "      <td>-1.003221</td>\n",
       "      <td>-0.857483</td>\n",
       "      <td>-0.243467</td>\n",
       "      <td>-0.643161</td>\n",
       "      <td>-0.769587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1700 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date       AUD       EUR       GBP       BRL       CAD       CNY  \\\n",
       "0    2018-01-03  0.334863 -0.362698 -0.962935  0.667124 -0.495991 -0.530372   \n",
       "1    2018-01-04  0.334211  0.656456  0.227988  0.509819  0.240629  0.542035   \n",
       "2    2018-01-05  0.074953 -0.456648  0.306200 -0.001248  2.406126  0.239796   \n",
       "3    2018-01-08 -0.403445 -1.234715  0.057436 -0.039856 -0.519592 -0.503176   \n",
       "4    2018-01-09 -0.304518 -0.955438 -0.623592 -0.553009 -0.613347 -1.587930   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "1695 2024-10-17  0.951873 -0.738979  0.114249 -0.262937 -0.312019 -0.100826   \n",
       "1696 2024-10-18 -0.031487  0.371217  0.195929 -0.007935 -0.208785  1.029334   \n",
       "1697 2024-10-21 -0.898234 -0.698279 -0.567894 -0.402437 -0.721574 -0.854682   \n",
       "1698 2024-10-22  0.437467 -0.300448 -0.145256  0.238558  0.304633 -0.186105   \n",
       "1699 2024-10-23 -1.089745 -0.532745 -0.337128 -0.204831 -0.122709 -0.090734   \n",
       "\n",
       "           INR       JPY       KRW  ...   ZAR_VaR   BTC_VaR   ETH_VaR  \\\n",
       "0    -0.145050 -0.129262 -0.635381  ... -1.496603 -0.965267 -0.921753   \n",
       "1     0.543494 -0.776700  0.317166  ... -1.483572 -0.935005 -1.066232   \n",
       "2     0.150080 -0.612152  0.285386  ... -1.452218 -0.897536 -1.031376   \n",
       "3    -0.440428  0.194247 -1.074947  ... -1.435233 -1.117968 -0.984176   \n",
       "4    -0.832109  0.857014 -0.421764  ... -1.430869 -1.393252 -1.440291   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1695 -0.059821 -0.465327 -0.985952  ... -1.401500 -1.103762 -0.984157   \n",
       "1696  0.088750  0.689939  0.227609  ... -1.384633 -1.061084 -0.917953   \n",
       "1697 -0.096920 -1.048106 -1.280341  ... -1.367547 -1.041412 -0.876093   \n",
       "1698  0.088737 -0.740440  0.018631  ... -1.363666 -1.034738 -0.825253   \n",
       "1699  0.051611 -2.005876 -0.298834  ... -1.353458 -0.985008 -0.834511   \n",
       "\n",
       "       XRP_VaR   BNB_VaR  DOGE_VaR   ADA_VaR   TRX_VaR   XLM_VaR  LINK_VaR  \n",
       "0    -0.734290 -0.911929 -0.922160 -0.891838 -0.213160 -0.630663 -0.841068  \n",
       "1    -1.079114 -1.024626 -0.850855 -1.215702 -0.847719 -1.228322 -0.786722  \n",
       "2    -1.074541 -1.101181 -0.876938 -1.201077 -3.730683 -1.288684 -1.338418  \n",
       "3    -1.070366 -2.199838 -1.409290 -1.361778 -3.403580 -1.292055 -1.395540  \n",
       "4    -1.057428 -2.494720 -1.713391 -1.503924 -4.612560 -1.280444 -1.943484  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1695 -0.807831 -0.972969 -0.964342 -0.904629 -0.296820 -0.680892 -0.839340  \n",
       "1696 -0.789106 -0.977637 -0.942883 -0.915365 -0.267755 -0.666286 -0.828467  \n",
       "1697 -0.775332 -0.940681 -1.004704 -0.897413 -0.257154 -0.670018 -0.826495  \n",
       "1698 -0.754175 -0.893332 -1.028174 -0.890601 -0.237957 -0.657851 -0.794404  \n",
       "1699 -0.746219 -0.893244 -1.003221 -0.857483 -0.243467 -0.643161 -0.769587  \n",
       "\n",
       "[1700 rows x 41 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([data, var_df], axis=1)\n",
    "data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98253d92-05c9-4eda-bd17-b92e28a6947d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9,\n",
       " 9,\n",
       " 9,\n",
       " [(0, 129),\n",
       "  (170, 299),\n",
       "  (340, 469),\n",
       "  (510, 639),\n",
       "  (680, 809),\n",
       "  (850, 979),\n",
       "  (1020, 1149),\n",
       "  (1190, 1319),\n",
       "  (1360, 1489)],\n",
       " [(130, 169),\n",
       "  (300, 339),\n",
       "  (470, 509),\n",
       "  (640, 679),\n",
       "  (810, 849),\n",
       "  (980, 1019),\n",
       "  (1150, 1189),\n",
       "  (1320, 1359),\n",
       "  (1490, 1529)],\n",
       " [(170, 339),\n",
       "  (340, 509),\n",
       "  (510, 679),\n",
       "  (680, 849),\n",
       "  (850, 1019),\n",
       "  (1020, 1189),\n",
       "  (1190, 1359),\n",
       "  (1360, 1529),\n",
       "  (1530, 1699)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define rolling window functions\n",
    "def create_sliding_windows(data, window_size=340, step_size=170):\n",
    "    train_windows = []\n",
    "    val_windows = []\n",
    "    test_windows = []\n",
    "    \n",
    "    for i in range(0, len(data) - window_size + 1, step_size):\n",
    "        window = data.iloc[i:i + window_size]\n",
    "        train_windows.append(window.iloc[:130])\n",
    "        val_windows.append(window.iloc[130:170])\n",
    "        test_windows.append(window.iloc[170:])\n",
    "    \n",
    "    return train_windows, val_windows, test_windows\n",
    "\n",
    "# Create training set, validation set and test set window\n",
    "train_windows, val_windows, test_windows = create_sliding_windows(data, window_size=340, step_size=170)\n",
    "\n",
    "# Print the number of windows\n",
    "train_window_count = len(train_windows)\n",
    "val_window_count = len(val_windows)\n",
    "test_window_count = len(test_windows)\n",
    "\n",
    "# Print the start and end positions of each window\n",
    "train_window_start_end = [(window.index[0], window.index[-1]) for window in train_windows]\n",
    "val_window_start_end = [(window.index[0], window.index[-1]) for window in val_windows]\n",
    "test_window_start_end = [(window.index[0], window.index[-1]) for window in test_windows]\n",
    "\n",
    "(train_window_count, val_window_count, test_window_count, train_window_start_end, val_window_start_end, test_window_start_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1007a7f3-2956-450d-b339-1bf4a3be3c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#Select CUDA or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb17be89-e429-4950-81f3-bd20054d06e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantileRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout_prob):\n",
    "        super(QuantileRegressionModel, self).__init__()\n",
    "        # Define the hidden layer\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(input_dim, hidden_dim) if i == 0 else nn.Linear(hidden_dim, hidden_dim) for i in range(num_layers)])\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim) # Define the output layer\n",
    "        self.activation = nn.ReLU()   # Use the ReLU activation function\n",
    "        self.dropout = nn.Dropout(dropout_prob)  # Use Dropout to prevent overfitting\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.hidden_layers:\n",
    "            x = self.activation(layer(x))  # Activation function applied to each hidden layer\n",
    "            x = self.dropout(x)  # Apply Dropout\n",
    "        x = self.output_layer(x)  # Final output layer\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define the objective function\n",
    "def quantile_loss(y_true, y_pred, tau=0.05):\n",
    "    diff = y_true - y_pred\n",
    "    loss = torch.where(diff < 0, (tau - 1) * diff, tau * diff)\n",
    "    return torch.mean(loss)\n",
    "\n",
    "def combined_loss(y_true, y_pred, model, lambda1=0.001, lambda2=0.001):\n",
    "    quantile = quantile_loss(y_true, y_pred)\n",
    "    l1_reg = lambda1 * sum(torch.sum(torch.abs(param)) for param in model.parameters())\n",
    "    l2_reg = lambda2 * sum(torch.sum(param**2) for param in model.parameters())\n",
    "    return quantile + l1_reg + l2_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63e86a93-b9b9-4775-8bd1-826b54bc11fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for AUD: {'hidden_dim': 13, 'num_layers': 1, 'dropout_prob': 0.5963102318903262, 'learning_rate': 0.037551726250202234, 'lambda1': 1.0562885651388624e-06, 'lambda2': 2.633742007580341e-05}\n",
      "Best validation loss for AUD: 0.0625670241812865\n",
      "All predictions for AUD: [[-2.3660636]\n",
      " [-2.3406763]\n",
      " [-2.2862048]\n",
      " ...\n",
      " [-2.795355 ]\n",
      " [-2.862184 ]\n",
      " [-2.7913663]]\n",
      "Shape of all_predictions_combined for AUD: (1530, 1)\n",
      "Average Validation Loss for AUD: 0.06434721044368213\n",
      "Average Test Loss for AUD: 0.13548284148176512\n",
      "Length of dates_list for AUD: 1530\n",
      "Length of CoVaR_list for AUD: 1530\n",
      "Best parameters for EUR: {'hidden_dim': 5, 'num_layers': 1, 'dropout_prob': 0.5168687055994572, 'learning_rate': 0.021302264902341942, 'lambda1': 1.3703793972945754e-05, 'lambda2': 3.6158297689014615e-05}\n",
      "Best validation loss for EUR: 0.06782035446829265\n",
      "All predictions for EUR: [[-3.2522812]\n",
      " [-3.090265 ]\n",
      " [-3.0561602]\n",
      " ...\n",
      " [-2.1219125]\n",
      " [-2.1490436]\n",
      " [-2.1201363]]\n",
      "Shape of all_predictions_combined for EUR: (1530, 1)\n",
      "Average Validation Loss for EUR: 0.06635840361316998\n",
      "Average Test Loss for EUR: 0.060480244664682284\n",
      "Length of dates_list for EUR: 1530\n",
      "Length of CoVaR_list for EUR: 1530\n",
      "Best parameters for GBP: {'hidden_dim': 13, 'num_layers': 1, 'dropout_prob': 0.5736097597676486, 'learning_rate': 0.004276401298990269, 'lambda1': 2.1596800400139047e-06, 'lambda2': 1.2275029609206662e-06}\n",
      "Best validation loss for GBP: 0.08001557696196768\n",
      "All predictions for GBP: [[-3.4287653]\n",
      " [-3.3038645]\n",
      " [-3.2161245]\n",
      " ...\n",
      " [-2.8858726]\n",
      " [-3.06755  ]\n",
      " [-2.9888752]]\n",
      "Shape of all_predictions_combined for GBP: (1530, 1)\n",
      "Average Validation Loss for GBP: 0.08606958720419142\n",
      "Average Test Loss for GBP: 0.11473561161094242\n",
      "Length of dates_list for GBP: 1530\n",
      "Length of CoVaR_list for GBP: 1530\n",
      "Best parameters for BRL: {'hidden_dim': 9, 'num_layers': 1, 'dropout_prob': 0.43807078736943256, 'learning_rate': 0.021628327467433334, 'lambda1': 7.256898983870502e-05, 'lambda2': 6.69924286893146e-06}\n",
      "Best validation loss for BRL: 0.09332853514287207\n",
      "All predictions for BRL: [[-3.7579248]\n",
      " [-3.808858 ]\n",
      " [-3.6028883]\n",
      " ...\n",
      " [-1.9343333]\n",
      " [-2.1035168]\n",
      " [-2.05994  ]]\n",
      "Shape of all_predictions_combined for BRL: (1530, 1)\n",
      "Average Validation Loss for BRL: 0.0957739647063944\n",
      "Average Test Loss for BRL: 0.0827793570028411\n",
      "Length of dates_list for BRL: 1530\n",
      "Length of CoVaR_list for BRL: 1530\n",
      "Best parameters for CAD: {'hidden_dim': 8, 'num_layers': 1, 'dropout_prob': 0.5974426297390805, 'learning_rate': 0.01808304654996736, 'lambda1': 3.0235880001585017e-06, 'lambda2': 2.1329199983232712e-06}\n",
      "Best validation loss for CAD: 0.07300044182274076\n",
      "All predictions for CAD: [[-3.0302012]\n",
      " [-3.0826368]\n",
      " [-2.908455 ]\n",
      " ...\n",
      " [-2.2001452]\n",
      " [-2.2024243]\n",
      " [-2.1697023]]\n",
      "Shape of all_predictions_combined for CAD: (1530, 1)\n",
      "Average Validation Loss for CAD: 0.07558984309434891\n",
      "Average Test Loss for CAD: 0.0776508781645033\n",
      "Length of dates_list for CAD: 1530\n",
      "Length of CoVaR_list for CAD: 1530\n",
      "Best parameters for CNY: {'hidden_dim': 66, 'num_layers': 1, 'dropout_prob': 0.48772928655334974, 'learning_rate': 0.003950444863047467, 'lambda1': 1.2057990613123076e-05, 'lambda2': 1.0149344376898259e-06}\n",
      "Best validation loss for CNY: 0.08424585892094506\n",
      "All predictions for CNY: [[-3.6488094]\n",
      " [-3.679784 ]\n",
      " [-3.564054 ]\n",
      " ...\n",
      " [-2.6264513]\n",
      " [-2.6848092]\n",
      " [-2.606659 ]]\n",
      "Shape of all_predictions_combined for CNY: (1530, 1)\n",
      "Average Validation Loss for CNY: 0.08296477090981272\n",
      "Average Test Loss for CNY: 0.11728126141760084\n",
      "Length of dates_list for CNY: 1530\n",
      "Length of CoVaR_list for CNY: 1530\n",
      "Best parameters for INR: {'hidden_dim': 6, 'num_layers': 1, 'dropout_prob': 0.4840927824349896, 'learning_rate': 0.01599360675561768, 'lambda1': 1.29125917178896e-06, 'lambda2': 2.8188016865641303e-06}\n",
      "Best validation loss for INR: 0.09689452333582772\n",
      "All predictions for INR: [[-3.208902 ]\n",
      " [-3.2702765]\n",
      " [-3.1801963]\n",
      " ...\n",
      " [-1.4783719]\n",
      " [-1.5086095]\n",
      " [-1.4721805]]\n",
      "Shape of all_predictions_combined for INR: (1530, 1)\n",
      "Average Validation Loss for INR: 0.10376377983225717\n",
      "Average Test Loss for INR: 0.09377719254957305\n",
      "Length of dates_list for INR: 1530\n",
      "Length of CoVaR_list for INR: 1530\n",
      "Best parameters for JPY: {'hidden_dim': 40, 'num_layers': 1, 'dropout_prob': 0.5963073310405899, 'learning_rate': 0.002985313903733915, 'lambda1': 1.7624410453834018e-06, 'lambda2': 1.2729716952502365e-06}\n",
      "Best validation loss for JPY: 0.08084759902622965\n",
      "All predictions for JPY: [[-2.305345 ]\n",
      " [-2.328454 ]\n",
      " [-2.2176995]\n",
      " ...\n",
      " [-2.8206198]\n",
      " [-2.9190216]\n",
      " [-2.8307407]]\n",
      "Shape of all_predictions_combined for JPY: (1530, 1)\n",
      "Average Validation Loss for JPY: 0.08155918411082691\n",
      "Average Test Loss for JPY: 0.12009094282984734\n",
      "Length of dates_list for JPY: 1530\n",
      "Length of CoVaR_list for JPY: 1530\n",
      "Best parameters for KRW: {'hidden_dim': 62, 'num_layers': 1, 'dropout_prob': 0.4092837134432671, 'learning_rate': 0.0015842094714411754, 'lambda1': 1.2933835502447464e-06, 'lambda2': 1.208313189327694e-05}\n",
      "Best validation loss for KRW: 0.08832429804735714\n",
      "All predictions for KRW: [[-4.6330304]\n",
      " [-4.732699 ]\n",
      " [-4.5163016]\n",
      " ...\n",
      " [-3.5212922]\n",
      " [-3.711188 ]\n",
      " [-3.6047285]]\n",
      "Shape of all_predictions_combined for KRW: (1530, 1)\n",
      "Average Validation Loss for KRW: 0.08845146662659115\n",
      "Average Test Loss for KRW: 0.08214725243548553\n",
      "Length of dates_list for KRW: 1530\n",
      "Length of CoVaR_list for KRW: 1530\n",
      "Best parameters for MXN: {'hidden_dim': 79, 'num_layers': 1, 'dropout_prob': 0.30676057764477016, 'learning_rate': 0.0018507505835013646, 'lambda1': 1.7539299565290831e-06, 'lambda2': 3.219792721452822e-05}\n",
      "Best validation loss for MXN: 0.07047789461082882\n",
      "All predictions for MXN: [[-3.4740257]\n",
      " [-3.4578788]\n",
      " [-3.3290792]\n",
      " ...\n",
      " [-3.3850265]\n",
      " [-3.396073 ]\n",
      " [-3.343683 ]]\n",
      "Shape of all_predictions_combined for MXN: (1530, 1)\n",
      "Average Validation Loss for MXN: 0.07216054076949756\n",
      "Average Test Loss for MXN: 0.2195311908920606\n",
      "Length of dates_list for MXN: 1530\n",
      "Length of CoVaR_list for MXN: 1530\n",
      "Best parameters for ZAR: {'hidden_dim': 91, 'num_layers': 1, 'dropout_prob': 0.28566951900676424, 'learning_rate': 0.0009564167666270214, 'lambda1': 6.8120126809707535e-06, 'lambda2': 4.8889514125799685e-05}\n",
      "Best validation loss for ZAR: 0.1061241411500507\n",
      "All predictions for ZAR: [[-3.6512556]\n",
      " [-3.6688485]\n",
      " [-3.4913158]\n",
      " ...\n",
      " [-3.6602666]\n",
      " [-3.863384 ]\n",
      " [-3.7447789]]\n",
      "Shape of all_predictions_combined for ZAR: (1530, 1)\n",
      "Average Validation Loss for ZAR: 0.11184666429956754\n",
      "Average Test Loss for ZAR: 0.098845517469777\n",
      "Length of dates_list for ZAR: 1530\n",
      "Length of CoVaR_list for ZAR: 1530\n",
      "Best parameters for BTC: {'hidden_dim': 45, 'num_layers': 1, 'dropout_prob': 0.24891274410908684, 'learning_rate': 0.0017468674301262897, 'lambda1': 7.11339208029672e-06, 'lambda2': 2.5076187259840096e-06}\n",
      "Best validation loss for BTC: 0.049854707386758595\n",
      "All predictions for BTC: [[-2.5070796]\n",
      " [-2.9098768]\n",
      " [-2.7543583]\n",
      " ...\n",
      " [-1.8066899]\n",
      " [-1.8280684]\n",
      " [-1.7973746]]\n",
      "Shape of all_predictions_combined for BTC: (1530, 1)\n",
      "Average Validation Loss for BTC: 0.053951376842127904\n",
      "Average Test Loss for BTC: 0.07642052653763029\n",
      "Length of dates_list for BTC: 1530\n",
      "Length of CoVaR_list for BTC: 1530\n",
      "Best parameters for ETH: {'hidden_dim': 9, 'num_layers': 1, 'dropout_prob': 0.47367418799773003, 'learning_rate': 0.013070167234398852, 'lambda1': 1.6408916610249368e-06, 'lambda2': 5.3033577114492654e-05}\n",
      "Best validation loss for ETH: 0.0522966424210204\n",
      "All predictions for ETH: [[-3.5636547]\n",
      " [-3.827011 ]\n",
      " [-3.6938767]\n",
      " ...\n",
      " [-1.2291851]\n",
      " [-1.1832527]\n",
      " [-1.1590866]]\n",
      "Shape of all_predictions_combined for ETH: (1530, 1)\n",
      "Average Validation Loss for ETH: 0.054547388520505696\n",
      "Average Test Loss for ETH: 0.06688270386722353\n",
      "Length of dates_list for ETH: 1530\n",
      "Length of CoVaR_list for ETH: 1530\n",
      "Best parameters for XRP: {'hidden_dim': 85, 'num_layers': 1, 'dropout_prob': 0.5666966518921189, 'learning_rate': 0.004548312409970763, 'lambda1': 3.024447824516878e-06, 'lambda2': 1.43282975847031e-06}\n",
      "Best validation loss for XRP: 0.04732278734445572\n",
      "All predictions for XRP: [[-2.4851782]\n",
      " [-2.8422134]\n",
      " [-2.6342664]\n",
      " ...\n",
      " [-1.687079 ]\n",
      " [-1.6611873]\n",
      " [-1.617736 ]]\n",
      "Shape of all_predictions_combined for XRP: (1530, 1)\n",
      "Average Validation Loss for XRP: 0.04861346114840773\n",
      "Average Test Loss for XRP: 0.06515411991212103\n",
      "Length of dates_list for XRP: 1530\n",
      "Length of CoVaR_list for XRP: 1530\n",
      "Best parameters for BNB: {'hidden_dim': 12, 'num_layers': 1, 'dropout_prob': 0.547434313846646, 'learning_rate': 0.027243002486001464, 'lambda1': 7.219689559328157e-05, 'lambda2': 1.3821823658267632e-05}\n",
      "Best validation loss for BNB: 0.06516494022475348\n",
      "All predictions for BNB: [[-3.8953116]\n",
      " [-4.4608717]\n",
      " [-4.325074 ]\n",
      " ...\n",
      " [-1.3937863]\n",
      " [-1.3235571]\n",
      " [-1.3102908]]\n",
      "Shape of all_predictions_combined for BNB: (1530, 1)\n",
      "Average Validation Loss for BNB: 0.07049897313117981\n",
      "Average Test Loss for BNB: 0.07050669276052052\n",
      "Length of dates_list for BNB: 1530\n",
      "Length of CoVaR_list for BNB: 1530\n",
      "Best parameters for DOGE: {'hidden_dim': 13, 'num_layers': 1, 'dropout_prob': 0.5903265052222557, 'learning_rate': 0.040343941732931585, 'lambda1': 7.410943755823188e-06, 'lambda2': 1.6787702374463902e-05}\n",
      "Best validation loss for DOGE: 0.06553148540357749\n",
      "All predictions for DOGE: [[-1.4305496]\n",
      " [-1.7472978]\n",
      " [-1.6468025]\n",
      " ...\n",
      " [-1.3960595]\n",
      " [-1.3565439]\n",
      " [-1.3234835]]\n",
      "Shape of all_predictions_combined for DOGE: (1530, 1)\n",
      "Average Validation Loss for DOGE: 0.06968745258119372\n",
      "Average Test Loss for DOGE: 0.058028324196736016\n",
      "Length of dates_list for DOGE: 1530\n",
      "Length of CoVaR_list for DOGE: 1530\n",
      "Best parameters for ADA: {'hidden_dim': 43, 'num_layers': 1, 'dropout_prob': 0.5661126144654598, 'learning_rate': 0.003722569688458704, 'lambda1': 2.5773425675094063e-06, 'lambda2': 6.77329664779905e-06}\n",
      "Best validation loss for ADA: 0.05532148645983802\n",
      "All predictions for ADA: [[-3.1264184]\n",
      " [-3.4437602]\n",
      " [-3.286868 ]\n",
      " ...\n",
      " [-1.6322521]\n",
      " [-1.6048998]\n",
      " [-1.5687298]]\n",
      "Shape of all_predictions_combined for ADA: (1530, 1)\n",
      "Average Validation Loss for ADA: 0.05777172909842597\n",
      "Average Test Loss for ADA: 0.06614699711402257\n",
      "Length of dates_list for ADA: 1530\n",
      "Length of CoVaR_list for ADA: 1530\n",
      "Best parameters for TRX: {'hidden_dim': 9, 'num_layers': 1, 'dropout_prob': 0.33968655164732936, 'learning_rate': 0.011525199820184318, 'lambda1': 7.106285275645513e-06, 'lambda2': 0.00015615664416403086}\n",
      "Best validation loss for TRX: 0.07212926716440254\n",
      "All predictions for TRX: [[-4.295836 ]\n",
      " [-4.5466104]\n",
      " [-4.3563   ]\n",
      " ...\n",
      " [-0.8994688]\n",
      " [-0.8588071]\n",
      " [-0.8565398]]\n",
      "Shape of all_predictions_combined for TRX: (1530, 1)\n",
      "Average Validation Loss for TRX: 0.07945332200162941\n",
      "Average Test Loss for TRX: 0.06391069748335415\n",
      "Length of dates_list for TRX: 1530\n",
      "Length of CoVaR_list for TRX: 1530\n",
      "Best parameters for XLM: {'hidden_dim': 36, 'num_layers': 1, 'dropout_prob': 0.515872055068463, 'learning_rate': 0.012301296231795199, 'lambda1': 1.0807140907567153e-06, 'lambda2': 2.6147601943588876e-06}\n",
      "Best validation loss for XLM: 0.04145693116717868\n",
      "All predictions for XLM: [[-3.3972476]\n",
      " [-3.7935278]\n",
      " [-3.6684363]\n",
      " ...\n",
      " [-1.8114372]\n",
      " [-1.8077533]\n",
      " [-1.7705674]]\n",
      "Shape of all_predictions_combined for XLM: (1530, 1)\n",
      "Average Validation Loss for XLM: 0.04407593918343385\n",
      "Average Test Loss for XLM: 0.055675425877173744\n",
      "Length of dates_list for XLM: 1530\n",
      "Length of CoVaR_list for XLM: 1530\n",
      "Best parameters for LINK: {'hidden_dim': 73, 'num_layers': 1, 'dropout_prob': 0.5670863863829809, 'learning_rate': 0.0013187472220339636, 'lambda1': 3.6137355497951085e-06, 'lambda2': 1.650806289473235e-05}\n",
      "Best validation loss for LINK: 0.06604041159152985\n",
      "All predictions for LINK: [[-3.6536167]\n",
      " [-4.046738 ]\n",
      " [-3.8641531]\n",
      " ...\n",
      " [-1.8838521]\n",
      " [-1.8676347]\n",
      " [-1.822239 ]]\n",
      "Shape of all_predictions_combined for LINK: (1530, 1)\n",
      "Average Validation Loss for LINK: 0.06879232202967007\n",
      "Average Test Loss for LINK: 0.07946986291143629\n",
      "Length of dates_list for LINK: 1530\n",
      "Length of CoVaR_list for LINK: 1530\n"
     ]
    }
   ],
   "source": [
    "#1. initial setup\n",
    "# List of names of currencies\n",
    "currencies =  vari_list\n",
    "best_model_state = {}  # Save the state and hyperparameters of the best model\n",
    "VaR_values = {}  # Save VaR values for each currency at different time periods\n",
    "\n",
    "# List of DataFrames storing the results of each currency's predictions\n",
    "results_df_list = []\n",
    "# Data set characteristics\n",
    "input_dim = len(vari_list)-1\n",
    "output_dim = 1\n",
    "\n",
    "for currency in currencies:\n",
    "    # Define the objective function for Bayesian optimisation\n",
    "    def objective(trial):\n",
    "        # Define the search space for hyperparameters here\n",
    "        hidden_dim = trial.suggest_int('hidden_dim', 1, 100)\n",
    "        num_layers = trial.suggest_int('num_layers', 1, 1)\n",
    "        dropout_prob = trial.suggest_float('dropout_prob', 0.1, 0.6)\n",
    "        learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
    "        lambda1 = trial.suggest_loguniform('lambda1', 1e-6, 1e-3)\n",
    "        lambda2 = trial.suggest_loguniform('lambda2', 1e-6, 1e-3)\n",
    "\n",
    "        # Create rolling windows\n",
    "        train_windows, val_windows, test_windows = create_sliding_windows(data, window_size=340, step_size=170)\n",
    "\n",
    "        val_losses = []\n",
    "        test_losses = []\n",
    "        all_predictions = []\n",
    "\n",
    "        for train_window, val_window, test_window in zip(train_windows, val_windows, test_windows):\n",
    "            model = QuantileRegressionModel(input_dim, hidden_dim, output_dim, num_layers, dropout_prob).to(device)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "            # Training and validation sets\n",
    "            Y_col = currency\n",
    "            X_cols = [c for c in currencies if c != currency]\n",
    "            train_X = torch.tensor(train_window[X_cols].values, dtype=torch.float32).to(device)\n",
    "            train_Y = torch.tensor(train_window[Y_col].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "            val_X = torch.tensor(val_window[X_cols].values, dtype=torch.float32).to(device)\n",
    "            val_Y = torch.tensor(val_window[Y_col].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "            # Test sets\n",
    "            test_X_cols = [f'{c}_VaR' for c in currencies if c != currency]\n",
    "            test_Y_col = f'{currency}_VaR'\n",
    "            test_X = torch.tensor(test_window[test_X_cols].values, dtype=torch.float32).to(device)\n",
    "            test_Y = torch.tensor(test_window[test_Y_col].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "            model.train()\n",
    "            for epoch in range(70):\n",
    "                optimizer.zero_grad()\n",
    "                predictions = model(train_X)\n",
    "                loss = combined_loss(train_Y, predictions, model, lambda1, lambda2)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_predictions = model(val_X)\n",
    "                val_loss = combined_loss(val_Y, val_predictions, model, lambda1, lambda2)\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "                test_predictions = model(test_X)\n",
    "                test_loss = combined_loss(test_Y, test_predictions, model, lambda1, lambda2)\n",
    "                test_losses.append(test_loss.item())\n",
    "                test_predictions_array = test_predictions.detach().cpu().numpy()\n",
    "                all_predictions.append(test_predictions_array)\n",
    "\n",
    "        # Save states and hyperparameters of the best model\n",
    "        best_model_state[currency] = {\n",
    "            'state_dict': model.state_dict(),\n",
    "            'params': {\n",
    "                'hidden_dim': hidden_dim,\n",
    "                'num_layers': num_layers,\n",
    "                'dropout_prob': dropout_prob,\n",
    "                'learning_rate': learning_rate,\n",
    "                'lambda1': lambda1,\n",
    "                'lambda2': lambda2\n",
    "            }\n",
    "        }\n",
    "        VaR_values[currency] = test_Y.cpu().numpy()  # Assuming test_Y is the VaR value\n",
    "\n",
    "        # Return average verification loss\n",
    "        return sum(val_losses) / len(val_losses)\n",
    "\n",
    "    # Create an Optuna study object for hyperparameter searching\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=50)  # Make 50 attempts\n",
    "\n",
    "    # Output the optimal result\n",
    "    print(f\"Best parameters for {currency}: {study.best_params}\")\n",
    "    print(f\"Best validation loss for {currency}: {study.best_value}\")\n",
    "\n",
    "    # Retrain the model using the found optimal hyperparameters\n",
    "    best_params = study.best_params\n",
    "\n",
    "    # Create rolling windows\n",
    "    train_windows, val_windows, test_windows = create_sliding_windows(data, window_size=340, step_size=170)\n",
    "\n",
    "    val_losses = []\n",
    "    test_losses = []\n",
    "    all_predictions = []\n",
    "\n",
    "    for train_window, val_window, test_window in zip(train_windows, val_windows, test_windows):\n",
    "        # Model instantiation\n",
    "        model = QuantileRegressionModel(input_dim, best_params['hidden_dim'], output_dim, best_params['num_layers'], best_params['dropout_prob']).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "        # Extract X and Y for training, validation and test sets\n",
    "        Y_col = currency\n",
    "        X_cols = [c for c in currencies if c != currency]\n",
    "        train_X = torch.tensor(train_window[X_cols].values, dtype=torch.float32).to(device)\n",
    "        train_Y = torch.tensor(train_window[Y_col].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "        val_X = torch.tensor(val_window[X_cols].values, dtype=torch.float32).to(device)\n",
    "        val_Y = torch.tensor(val_window[Y_col].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "        test_X_cols = [f'{c}_VaR' for c in currencies if c != currency]\n",
    "        test_Y_col = f'{currency}_VaR'\n",
    "        test_X = torch.tensor(test_window[test_X_cols].values, dtype=torch.float32).to(device)\n",
    "        test_Y = torch.tensor(test_window[test_Y_col].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "        # Training models\n",
    "        model.train()\n",
    "        for epoch in range(70):  # 70 rounds of training\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(train_X)\n",
    "            loss = combined_loss(train_Y, predictions, model, best_params['lambda1'], best_params['lambda2'])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "          \n",
    "\n",
    "        # Validate the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_predictions = model(val_X)\n",
    "            val_loss = combined_loss(val_Y, val_predictions, model, best_params['lambda1'], best_params['lambda2'])\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "            # Test model performance\n",
    "            test_predictions = model(test_X)\n",
    "            test_loss = combined_loss(test_Y, test_predictions, model, best_params['lambda1'], best_params['lambda2'])\n",
    "            test_losses.append(test_loss.item())\n",
    "            test_predictions_array = test_predictions.detach().cpu().numpy()\n",
    "            all_predictions.append(test_predictions_array)\n",
    "\n",
    "    # Use np.concatenate to stitch together all predictions\n",
    "    all_predictions_combined = np.concatenate(all_predictions, axis=0)\n",
    "\n",
    "    # Print the spliced result and its shape\n",
    "    print(f\"All predictions for {currency}:\", all_predictions_combined)\n",
    "    print(f\"Shape of all_predictions_combined for {currency}:\", all_predictions_combined.shape)\n",
    "\n",
    "    # Output results\n",
    "    best_model_state[currency]['val_losses_nn'] = val_losses\n",
    "    best_model_state[currency]['test_losses_nn'] = test_losses\n",
    "    print(f\"Average Validation Loss for {currency}: {sum(val_losses) / len(val_losses)}\")\n",
    "    print(f\"Average Test Loss for {currency}: {sum(test_losses) / len(test_losses)}\")\n",
    "\n",
    "        # Create lists storing predicted values and dates\n",
    "    CoVaR_list = []\n",
    "    dates_list = []\n",
    "\n",
    "    # Extract predicted values and dates for each test window\n",
    "    for test_window, test_predictions in zip(test_windows, all_predictions):\n",
    "        dates = test_window['date']  # 获取当前窗口的日期\n",
    "        dates_list.extend(dates)\n",
    "        CoVaR_list.extend(test_predictions.flatten())  # Expand and add predictions to the list\n",
    "\n",
    "    # Print a list of dates and their lengths\n",
    "    print(f\"Length of dates_list for {currency}:\", len(dates_list))\n",
    "    print(f\"Length of CoVaR_list for {currency}:\", len(CoVaR_list))\n",
    "\n",
    "    # Create a DataFrame with dates and projected values\n",
    "    CoVaR_df = pd.DataFrame({\n",
    "        'date': dates_list,\n",
    "        f'{currency}_CoVaR': CoVaR_list\n",
    "    })\n",
    "\n",
    "    # :: Ensure consistency in date format of raw and projected data\n",
    "    CoVaR_df['date'] = pd.to_datetime(CoVaR_df['date'])\n",
    "\n",
    "    # Add results to results list\n",
    "    results_df_list.append(CoVaR_df)\n",
    "\n",
    "# Merge all company forecasts into the original dataset\n",
    "for result_df in results_df_list:\n",
    "    data = pd.merge(data, result_df, on='date', how='left')\n",
    "\n",
    "# View the merged dataset\n",
    "data = data.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d786f28e-c6da-45f0-9bcb-45d65df77872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the linear regression model\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "input=len(vari_list)-1\n",
    "for currency in currencies:\n",
    "    # Create rolling windows\n",
    "    train_windows, val_windows, test_windows = create_sliding_windows(data, window_size=340, step_size=170)\n",
    "    \n",
    "    val_losses_lr = []\n",
    "    test_losses_lr = []\n",
    "    all_predictions = []\n",
    "    for train_window, val_window, test_window in zip(train_windows, val_windows, test_windows):\n",
    "        # Instantiated models\n",
    "        model = LinearRegressionModel(input)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "        # Extract X and Y for training, validation and test sets\n",
    "        Y_col = currency\n",
    "        X_cols = [c for c in currencies if c != currency]\n",
    "        train_X = torch.tensor(train_window[X_cols].values, dtype=torch.float32).to(device)\n",
    "        train_Y = torch.tensor(train_window[Y_col].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "        val_X = torch.tensor(val_window[X_cols].values, dtype=torch.float32).to(device)\n",
    "        val_Y = torch.tensor(val_window[Y_col].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "        test_X_cols = [f'{c}_VaR' for c in currencies if c != currency]\n",
    "        test_Y_col = f'{currency}_VaR'\n",
    "        test_X = torch.tensor(test_window[test_X_cols].values, dtype=torch.float32).to(device)\n",
    "        test_Y = torch.tensor(test_window[test_Y_col].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "        # Training models\n",
    "        model.train()\n",
    "        for epoch in range(70):  # 100 rounds of training\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(train_X)\n",
    "            loss = combined_loss(train_Y, predictions, model,lambda1=0.001, lambda2=0.001)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # Validate the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_predictions = model(val_X)\n",
    "            val_loss_lr = combined_loss(val_Y, val_predictions, model,lambda1=0.001, lambda2=0.001)\n",
    "            val_losses_lr.append(val_loss_lr.item())\n",
    "            \n",
    "\n",
    "            # Test model performance\n",
    "            test_predictions = model(test_X)\n",
    "            test_loss_lr = combined_loss(test_Y, test_predictions, model,lambda1=0.001, lambda2=0.001)\n",
    "            test_losses_lr.append(test_loss_lr.item())\n",
    "            test_predictions_array = test_predictions.detach().cpu().numpy()\n",
    "            all_predictions.append(test_predictions_array)\n",
    "\n",
    "    # Use np.concatenate to stitch together all predictions\n",
    "    all_predictions_combined = np.concatenate(all_predictions, axis=0)\n",
    "    best_model_state[currency]['val_losses_lr'] = val_losses_lr\n",
    "    best_model_state[currency]['test_losses_lr'] = test_losses_lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ec13b52-d0e3-4e95-8fa5-e091de23f273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diebold_mariano_test(errors_nn, errors_lr):\n",
    "    # Ensure that both error arrays have the same length\n",
    "    min_len = min(len(errors_nn), len(errors_lr))\n",
    "    errors_nn = errors_nn[:min_len]\n",
    "    errors_lr = errors_lr[:min_len]\n",
    "    errors_nn = np.array(errors_nn)\n",
    "    errors_lr = np.array(errors_lr)\n",
    "\n",
    "    # Calculate the average error\n",
    "    mean_nn = np.mean(errors_nn)\n",
    "    mean_lr = np.mean(errors_lr)\n",
    "    \n",
    "    # Differences in calculation errors\n",
    "    error_diff = errors_nn - errors_lr\n",
    "    \n",
    "    # Calculate the variance of the difference (using the whole series)\n",
    "    var_diff = np.var(error_diff, ddof=1)\n",
    "    \n",
    "    # Calculate DM statistics\n",
    "    DM_statistic = (mean_nn - mean_lr) / np.sqrt(var_diff / min_len)\n",
    "    \n",
    "    # Calculate the p-value\n",
    "    p_value = 2 * (1 - norm.cdf(abs(DM_statistic)))\n",
    "    \n",
    "    return DM_statistic, p_value\n",
    "\n",
    "DM_results = {}\n",
    "for currency in currencies:\n",
    "    DM_statistic, p_value = diebold_mariano_test( best_model_state[currency]['val_losses_nn'], best_model_state[currency]['val_losses_lr'])\n",
    "    DM_results[currency] = {'DM Statistic': DM_statistic, 'p-value': p_value}\n",
    "\n",
    "df_DM_results = pd.DataFrame.from_dict(DM_results, orient='index')\n",
    "\n",
    "# Rename columns\n",
    "df_DM_results.columns = ['DM Statistic', 'p-value']\n",
    "\n",
    "# Add the currency name as the first column to the data frame\n",
    "df_DM_results.reset_index(inplace=True)\n",
    "df_DM_results.rename(columns={'index': 'currencies'}, inplace=True)\n",
    "df_DM_results = df_DM_results.T\n",
    "df_DM_results.to_excel('DM_test_95.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b146994a-a05a-4e3b-9996-ef61cf8bfc33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n"
     ]
    }
   ],
   "source": [
    "# Define functions to calculate risk spillovers\n",
    "def calculate_spillover_effects(j, currency, model_state, VaR_values):\n",
    "    # Extract model weights\n",
    "    best_params = model_state[currency]['params']\n",
    "    model = QuantileRegressionModel(input_dim, best_params['hidden_dim'], output_dim, best_params['num_layers'], best_params['dropout_prob']).to(device)\n",
    "    model.load_state_dict(model_state[currency]['state_dict'])\n",
    "    \n",
    "    # Get linear layer weights for the model\n",
    "    linear_weights = np.array(model.output_layer.weight.data.cpu().numpy())\n",
    "\n",
    "    # Get intermediate layer weights and biases for the model\n",
    "    hidden_weights = [layer.weight.data.cpu().numpy() for layer in model.hidden_layers]\n",
    "    hidden_biases = [layer.bias.data.cpu().numpy() for layer in model.hidden_layers]\n",
    "\n",
    "    # Initialise arrays storing risk spillage effects\n",
    "    effects = [0] * len(currencies)\n",
    "\n",
    "    for i, target in enumerate (currencies):\n",
    "        if i<j :\n",
    "            effect = 0\n",
    "            for m in range(len(hidden_weights[0])):\n",
    "                wh_m = np.array(hidden_weights[0][m,])\n",
    "                bh_m = np.array(hidden_biases[0][m,])\n",
    "                VaRvector = np.array([VaR_values[key] for key in VaR_values if key != target])\n",
    "                input_sum = np.dot(wh_m, VaRvector)+bh_m\n",
    "                if input_sum.all() > 0:\n",
    "                    effect += linear_weights[0,m]*input_sum*wh_m[i]\n",
    "                effect = np.abs(effect)\n",
    "            effects[i] = effect\n",
    "        else:\n",
    "            if i > j:\n",
    "                effect = 0\n",
    "                p = i-1\n",
    "                for m in range(len(hidden_weights[0])):\n",
    "                    wh_m = np.array(hidden_weights[0][m,])\n",
    "                    bh_m = np.array(hidden_biases[0][m,])\n",
    "                    VaRvector = np.array([VaR_values[key] for key in VaR_values if key != target])\n",
    "                    input_sum = np.dot(wh_m, VaRvector)+bh_m\n",
    "                    if input_sum.all() > 0:\n",
    "                        effect += linear_weights[0,m]*input_sum*wh_m[p]\n",
    "                    effect = np.abs(effect)\n",
    "                effects[i] = effect\n",
    "            else:\n",
    "                effects[i] = 0\n",
    "\n",
    "    return effects\n",
    "\n",
    "# Store risk spillover matrix at all points in time\n",
    "spillover_matrix = []\n",
    "\n",
    "# Traversing each point in time\n",
    "for t in range(len(data)):\n",
    "    VaR_values_t = {currency: data.iloc[t][f'{currency}_VaR'] for currency in currencies}\n",
    "    spillover_matrix_t = np.zeros((len(currencies), len(currencies)))\n",
    "\n",
    "    for j, target_currency in enumerate(currencies):\n",
    "        spillover_matrix_t[j] = calculate_spillover_effects(j, target_currency, best_model_state, VaR_values_t)\n",
    "    \n",
    "    spillover_matrix.append(spillover_matrix_t)\n",
    "    print(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1f7dd717-1ad1-4c54-b5e5-6fb4c3e2a7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export CoVaR estimation results\n",
    "data.to_csv('outputdata.csv', index=False)\n",
    "\n",
    "# Export the estimation of the adjacency matrix\n",
    "spillover_matrix=np.array(spillover_matrix)\n",
    "np.save('spillover_matrix',spillover_matrix) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
